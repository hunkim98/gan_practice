{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log10 # For metric function\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://velog.io/@wilko97/%EB%85%BC%EB%AC%B8%EC%8B%A4%EC%8A%B5-Pix2Pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split concatted image into two images\n",
    "def split_image(image):\n",
    "    width, height = image.size\n",
    "    width = int(width / 2)\n",
    "    left = image.crop((0, 0, width, height))\n",
    "    right = image.crop((width, 0, width * 2, height))\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset from ImageFolder\n",
    "class Dataset(data.Dataset): # torch기본 Dataset 상속받기\n",
    "    def __init__(self, image_dir, direction):\n",
    "        super(Dataset, self).__init__() # 초기화 상속\n",
    "        self.direction = direction # \n",
    "        self.a_path = os.path.join(image_dir, \"a\") # a는 건물 사진\n",
    "        self.b_path = os.path.join(image_dir, \"b\") # b는 Segmentation Mask\n",
    "        self.image_filenames = [x for x in os.listdir(self.a_path)] # a 폴더에 있는 파일 목록\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 256)), # 이미지 크기 조정\n",
    "                                            transforms.ToTensor(), # Numpy -> Tensor\n",
    "                                             transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                                std=(0.5, 0.5, 0.5)) # Normalization : -1 ~ 1 range\n",
    "                                            ])\n",
    "        self.len = len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # 건물사진과 Segmentation mask를 각각 a,b 폴더에서 불러오기\n",
    "        a = Image.open(os.path.join(self.a_path, self.image_filenames[index])).convert('RGB') # 건물 사진\n",
    "        b = Image.open(os.path.join(self.b_path, self.image_filenames[index])).convert('RGB') # Segmentation 사진\n",
    "        \n",
    "        # 이미지 전처리\n",
    "        a = self.transform(a)\n",
    "        b = self.transform(b)\n",
    "        \n",
    "        if self.direction == \"a2b\": # 건물 -> Segmentation\n",
    "            return a, b\n",
    "        else:  # Segmentation -> 건물\n",
    "            return b, a\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "train_dataset = Dataset(\"./data/fonts/train/\", \"b2a\")\n",
    "test_dataset = Dataset(\"./data/fonts/test/\", \"b2a\")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, num_workers=0, batch_size=1, shuffle=True) # Shuffle\n",
    "test_loader = DataLoader(dataset=test_dataset, num_workers=0, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 ~ 1사이의 값을 0~1사이로 만들어준다\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "# 이미지 시각화 함수\n",
    "def show_images(real_a, real_b, fake_b):\n",
    "    plt.figure(figsize=(30,90))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(real_a.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(real_b.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(fake_b.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv -> Batchnorm -> Activate function Layer\n",
    "'''\n",
    "코드 단순화를 위한 convolution block 생성을 위한 함수\n",
    "Encoder에서 사용될 예정\n",
    "'''\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True, activation='relu'):\n",
    "    layers = []\n",
    "    \n",
    "    # Conv layer\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    \n",
    "    # Batch Normalization\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    \n",
    "    # Activation\n",
    "    if activation == 'lrelu':\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    elif activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'none':\n",
    "        layers.append(nn.Sigmoid())\n",
    "        pass\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Deconv -> BatchNorm -> Activate function Layer\n",
    "'''\n",
    "코드 단순화를 위한 convolution block 생성을 위한 함수\n",
    "Decoder에서 이미지 복원을 위해 사용될 예정\n",
    "'''\n",
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True, activation='lrelu'):\n",
    "    layers = []\n",
    "    \n",
    "    # Deconv.\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    \n",
    "    # Batchnorm\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    \n",
    "    # Activation\n",
    "    if activation == 'lrelu':\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    elif activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'none':\n",
    "        pass\n",
    "                \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Unet encoder\n",
    "        self.conv1 = conv(3, 64, 4, bn=False, activation='lrelu') # (B, 64, 128, 128)\n",
    "        self.conv2 = conv(64, 128, 4, activation='lrelu') # (B, 128, 64, 64)\n",
    "        self.conv3 = conv(128, 256, 4, activation='lrelu') # (B, 256, 32, 32)\n",
    "        self.conv4 = conv(256, 512, 4, activation='lrelu') # (B, 512, 16, 16)\n",
    "        self.conv5 = conv(512, 512, 4, activation='lrelu') # (B, 512, 8, 8)\n",
    "        self.conv6 = conv(512, 512, 4, activation='lrelu') # (B, 512, 4, 4)\n",
    "        self.conv7 = conv(512, 512, 4, activation='lrelu') # (B, 512, 2, 2)\n",
    "        self.conv8 = conv(512, 512, 4, bn=False, activation='relu') # (B, 512, 1, 1)\n",
    "\n",
    "        # Unet decoder\n",
    "        self.deconv1 = deconv(512, 512, 4, activation='relu') # (B, 512, 2, 2)\n",
    "        self.deconv2 = deconv(1024, 512, 4, activation='relu') # (B, 512, 4, 4)\n",
    "        self.deconv3 = deconv(1024, 512, 4, activation='relu') # (B, 512, 8, 8) # Hint : U-Net에서는 Encoder에서 넘어온 Feature를 Concat합니다! (Channel이 2배)\n",
    "        self.deconv4 = deconv(1024, 512, 4, activation='relu') # (B, 512, 16, 16)\n",
    "        self.deconv5 = deconv(1024, 256, 4, activation='relu') # (B, 256, 32, 32)\n",
    "        self.deconv6 = deconv(512, 128, 4, activation='relu') # (B, 128, 64, 64)\n",
    "        self.deconv7 = deconv(256, 64, 4, activation='relu') # (B, 64, 128, 128)\n",
    "        self.deconv8 = deconv(128, 3, 4, activation='tanh') # (B, 3, 256, 256)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        # Unet encoder\n",
    "        e1 = self.conv1(input)\n",
    "        e2 = self.conv2(e1)\n",
    "        e3 = self.conv3(e2)\n",
    "        e4 = self.conv4(e3)\n",
    "        e5 = self.conv5(e4)\n",
    "        e6 = self.conv6(e5)\n",
    "        e7 = self.conv7(e6)\n",
    "        e8 = self.conv8(e7)\n",
    "                              \n",
    "        # Unet decoder\n",
    "        d1 = F.dropout(self.deconv1(e8), 0.5, training=True)\n",
    "        d2 = F.dropout(self.deconv2(torch.cat([d1, e7], 1)), 0.5, training=True)\n",
    "        d3 = F.dropout(self.deconv3(torch.cat([d2, e6], 1)), 0.5, training=True)\n",
    "        d4 = self.deconv4(torch.cat([d3, e5], 1))\n",
    "        d5 = self.deconv5(torch.cat([d4, e4], 1))\n",
    "        d6 = self.deconv6(torch.cat([d5, e3], 1))\n",
    "        d7 = self.deconv7(torch.cat([d6, e2], 1))\n",
    "        output = self.deconv8(torch.cat([d7, e1], 1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = conv(6, 64, 4, bn=False, activation='lrelu')\n",
    "        self.conv2 = conv(64, 128, 4, activation='lrelu')\n",
    "        self.conv3 = conv(128, 256, 4, activation='lrelu')\n",
    "        self.conv4 = conv(256, 512, 4, 1, 1, activation='lrelu')\n",
    "        self.conv5 = conv(512, 1, 4, 1, 1, activation='none')\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd3/hunkim98/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1, 30, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Backprop + Optimize\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m D\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m loss_d\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m d_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m#=============== Train the generator ===============#\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# First, G(A) should fake the discriminator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/autograd/__init__.py:150\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    146\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    147\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    149\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 150\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_)\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/autograd/__init__.py:51\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# Generator와 Discriminator를 GPU로 보내기\n",
    "G = Generator().cuda()\n",
    "D = Discriminator().cuda()\n",
    "\n",
    "criterionL1 = nn.L1Loss().cuda()\n",
    "criterionMSE = nn.MSELoss().cuda()\n",
    "criterionBCE = nn.BCELoss().cuda()\n",
    "\n",
    "# Setup optimizer\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "epoch_range = 3\n",
    "# Train\n",
    "for epoch in range(1, epoch_range):\n",
    "    for i, (real_a, real_b) in enumerate(train_loader, 1):\n",
    "        # forward\n",
    "        real_a, real_b = real_a.cuda(), real_b.cuda()\n",
    "        real_label = torch.ones(1).cuda()\n",
    "        fake_label = torch.zeros(1).cuda()\n",
    "        \n",
    "        fake_b = G(real_a) # G가 생성한 fake Segmentation mask\n",
    "        \n",
    "        #============= Train the discriminator =============#\n",
    "        # train with fake\n",
    "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "        pred_fake = D.forward(fake_ab.detach())\n",
    "        # print(pred_fake)\n",
    "\n",
    "        mse = (pred_fake - 0) ** 2\n",
    "\n",
    "        # print('mse : ', mse.mean().item())\n",
    "        loss_d_fake = criterionMSE(pred_fake, fake_label)\n",
    "        # print('loss_d_fake : ', loss_d_fake.item())\n",
    "        # break\n",
    "\n",
    "        # train with real\n",
    "        real_ab = torch.cat((real_a, real_b), 1)\n",
    "        pred_real = D.forward(real_ab)\n",
    "        loss_d_real = criterionMSE(pred_real, real_label)\n",
    "        \n",
    "        # Combined D loss\n",
    "        loss_d = (loss_d_fake + loss_d_real) * 0.5\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        D.zero_grad()\n",
    "        loss_d.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        #=============== Train the generator ===============#\n",
    "        # First, G(A) should fake the discriminator\n",
    "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "        pred_fake = D.forward(fake_ab)\n",
    "        loss_g_gan = criterionMSE(pred_fake, real_label)\n",
    "        pred_fake_mean = pred_fake.mean()\n",
    "        mse_value = (pred_fake - real_label) ** 2\n",
    "        # print('pred_fake_mean : ', mse_value.mean().item())\n",
    "        # print('loss_g_gan : ', loss_g_gan.item())\n",
    "\n",
    "        # Second, G(A) = B\n",
    "        loss_g_l1 = criterionL1(fake_b, real_label) * 10\n",
    "        \n",
    "        # loss_g = loss_g_gan + loss_g_l1\n",
    "        loss_g = loss_g_gan \n",
    "        # print('loss_g : ', loss_g.item())\n",
    "        # break\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "        loss_g.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('======================================================================================================')\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f'\n",
    "                  % (epoch, epoch_range, i, len(train_loader), loss_d.item(), loss_g.item()))\n",
    "            print('======================================================================================================')\n",
    "            show_images(denorm(real_a.squeeze()), denorm(real_b.squeeze()), denorm(fake_b.squeeze()))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
