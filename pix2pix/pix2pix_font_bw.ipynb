{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log10 # For metric function\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "print(is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://velog.io/@wilko97/%EB%85%BC%EB%AC%B8%EC%8B%A4%EC%8A%B5-Pix2Pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset from ImageFolder\n",
    "class Dataset(data.Dataset): # torch기본 Dataset 상속받기\n",
    "    def __init__(self, image_dir, direction):\n",
    "        super(Dataset, self).__init__() # 초기화 상속\n",
    "        self.direction = direction # \n",
    "        self.a_path = os.path.join(image_dir, \"a\") # a는 건물 사진\n",
    "        self.b_path = os.path.join(image_dir, \"b\") # b는 Segmentation Mask\n",
    "        self.image_filenames = [x for x in os.listdir(self.a_path)] # a 폴더에 있는 파일 목록\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 256)), # 이미지 크기 조정\n",
    "                                             transforms.Grayscale(),\n",
    "                                            transforms.ToTensor(), # Numpy -> Tensor\n",
    "                                             transforms.Normalize(mean=(0.5,), \n",
    "                                                std=(0.5,)) # Normalization : -1 ~ 1 range\n",
    "                                            ])\n",
    "        self.len = len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # 건물사진과 Segmentation mask를 각각 a,b 폴더에서 불러오기\n",
    "        a = Image.open(os.path.join(self.a_path, self.image_filenames[index])).convert('L') # 건물 사진\n",
    "        b = Image.open(os.path.join(self.b_path, self.image_filenames[index])).convert('L') # Segmentation 사진\n",
    "        \n",
    "        # 이미지 전처리\n",
    "        a = self.transform(a)\n",
    "        b = self.transform(b)\n",
    "        \n",
    "        if self.direction == \"a2b\": # 건물 -> Segmentation\n",
    "            return a, b\n",
    "        else:  # Segmentation -> 건물\n",
    "            return b, a\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "train_dataset = Dataset(\"./data/fonts/train/\", \"b2a\")\n",
    "test_dataset = Dataset(\"./data/fonts/test/\", \"b2a\")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, num_workers=0, batch_size=32, shuffle=True) # Shuffle\n",
    "test_loader = DataLoader(dataset=test_dataset, num_workers=0, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 ~ 1사이의 값을 0~1사이로 만들어준다\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "# 이미지 시각화 함수\n",
    "def show_images(real_a, real_b, fake_b):\n",
    "    plt.figure(figsize=(30,90))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(real_a.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(real_b.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(fake_b.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv -> Batchnorm -> Activate function Layer\n",
    "'''\n",
    "코드 단순화를 위한 convolution block 생성을 위한 함수\n",
    "Encoder에서 사용될 예정\n",
    "'''\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True, activation='relu'):\n",
    "    layers = []\n",
    "    \n",
    "    # Conv layer\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    \n",
    "    # Batch Normalization\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    \n",
    "    # Activation\n",
    "    if activation == 'lrelu':\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    elif activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'none':\n",
    "        pass\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Deconv -> BatchNorm -> Activate function Layer\n",
    "'''\n",
    "코드 단순화를 위한 convolution block 생성을 위한 함수\n",
    "Decoder에서 이미지 복원을 위해 사용될 예정\n",
    "'''\n",
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True, activation='lrelu'):\n",
    "    layers = []\n",
    "    \n",
    "    # Deconv.\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    \n",
    "    # Batchnorm\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    \n",
    "    # Activation\n",
    "    if activation == 'lrelu':\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    elif activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'none':\n",
    "        pass\n",
    "                \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Unet encoder\n",
    "        self.conv1 = conv(1, 64, 4, bn=False, activation='lrelu') # (B, 64, 128, 128)\n",
    "        self.conv2 = conv(64, 128, 4, activation='lrelu') # (B, 128, 64, 64)\n",
    "        self.conv3 = conv(128, 256, 4, activation='lrelu') # (B, 256, 32, 32)\n",
    "        self.conv4 = conv(256, 512, 4, activation='lrelu') # (B, 512, 16, 16)\n",
    "        self.conv5 = conv(512, 512, 4, activation='lrelu') # (B, 512, 8, 8)\n",
    "        self.conv6 = conv(512, 512, 4, activation='lrelu') # (B, 512, 4, 4)\n",
    "        self.conv7 = conv(512, 512, 4, activation='lrelu') # (B, 512, 2, 2)\n",
    "        self.conv8 = conv(512, 512, 4, bn=False, activation='relu') # (B, 512, 1, 1)\n",
    "\n",
    "        # Unet decoder\n",
    "        self.deconv1 = deconv(512, 512, 4, activation='relu') # (B, 512, 2, 2)\n",
    "        self.deconv2 = deconv(1024, 512, 4, activation='relu') # (B, 512, 4, 4)\n",
    "        self.deconv3 = deconv(1024, 512, 4, activation='relu') # (B, 512, 8, 8) # Hint : U-Net에서는 Encoder에서 넘어온 Feature를 Concat합니다! (Channel이 2배)\n",
    "        self.deconv4 = deconv(1024, 512, 4, activation='relu') # (B, 512, 16, 16)\n",
    "        self.deconv5 = deconv(1024, 256, 4, activation='relu') # (B, 256, 32, 32)\n",
    "        self.deconv6 = deconv(512, 128, 4, activation='relu') # (B, 128, 64, 64)\n",
    "        self.deconv7 = deconv(256, 64, 4, activation='relu') # (B, 64, 128, 128)\n",
    "        self.deconv8 = deconv(128, 1, 4, activation='tanh') # (B, 1, 256, 256)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Unet encoder\n",
    "        e1 = self.conv1(input)\n",
    "        e2 = self.conv2(e1)\n",
    "        e3 = self.conv3(e2)\n",
    "        e4 = self.conv4(e3)\n",
    "        e5 = self.conv5(e4)\n",
    "        e6 = self.conv6(e5)\n",
    "        e7 = self.conv7(e6)\n",
    "        e8 = self.conv8(e7)\n",
    "                              \n",
    "        # Unet decoder\n",
    "        d1 = F.dropout(self.deconv1(e8), 0.5, training=True)\n",
    "        d2 = F.dropout(self.deconv2(torch.cat([d1, e7], 1)), 0.5, training=True)\n",
    "        d3 = F.dropout(self.deconv3(torch.cat([d2, e6], 1)), 0.5, training=True)\n",
    "        d4 = self.deconv4(torch.cat([d3, e5], 1))\n",
    "        d5 = self.deconv5(torch.cat([d4, e4], 1))\n",
    "        d6 = self.deconv6(torch.cat([d5, e3], 1))\n",
    "        d7 = self.deconv7(torch.cat([d6, e2], 1))\n",
    "        output = self.deconv8(torch.cat([d7, e1], 1))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = conv(2, 64, 4, bn=False, activation='lrelu')\n",
    "        self.conv2 = conv(64, 128, 4, activation='lrelu')\n",
    "        self.conv3 = conv(128, 256, 4, activation='lrelu')\n",
    "        self.conv4 = conv(256, 512, 4, 1, 1, activation='lrelu')\n",
    "        self.conv5 = conv(512, 1, 4, 1, 1, activation='none')\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd3/hunkim98/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([32, 1, 30, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epoch_range):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (real_a, real_b) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader, \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         real_a, real_b \u001b[39m=\u001b[39m real_a\u001b[39m.\u001b[39mcuda(), real_b\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         real_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m b \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_path, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_filenames[index]))\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Segmentation 사진\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# 이미지 전처리\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(a)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(b)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.hcil.snu.ac.kr/ssd3/hunkim98/gan_practice/pix2pix/pix2pix_font_bw.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirection \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma2b\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m# 건물 -> Segmentation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torchvision/transforms/transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 61\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torchvision/transforms/transforms.py:226\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    219\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2pix/lib/python3.8/site-packages/torchvision/transforms/functional.py:340\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    337\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(tensor\u001b[39m.\u001b[39msize()))\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n\u001b[0;32m--> 340\u001b[0m     tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mclone()\n\u001b[1;32m    342\u001b[0m dtype \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    343\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mean, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generator와 Discriminator를 GPU로 보내기\n",
    "G = Generator().cuda()\n",
    "D = Discriminator().cuda()\n",
    "\n",
    "criterionL1 = nn.L1Loss().cuda()\n",
    "criterionMSE = nn.MSELoss().cuda()\n",
    "\n",
    "# Setup optimizer\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "epoch_range = 2\n",
    "\n",
    "batch_size = train_loader.batch_size\n",
    "print(batch_size)\n",
    "# Train\n",
    "for epoch in range(1, epoch_range):\n",
    "    for i, (real_a, real_b) in enumerate(train_loader, 1):\n",
    "        # forward\n",
    "        real_a, real_b = real_a.cuda(), real_b.cuda()\n",
    "        real_label = torch.ones(1).cuda()\n",
    "        fake_label = torch.zeros(1).cuda()\n",
    "        \n",
    "        fake_b = G(real_a) # G가 생성한 fake Segmentation mask\n",
    "        \n",
    "        #============= Train the discriminator =============#\n",
    "        # train with fake\n",
    "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "        pred_fake = D.forward(fake_ab.detach())\n",
    "        loss_d_fake = criterionMSE(pred_fake, fake_label)\n",
    "\n",
    "        # train with real\n",
    "        real_ab = torch.cat((real_a, real_b), 1)\n",
    "        pred_real = D.forward(real_ab)\n",
    "        loss_d_real = criterionMSE(pred_real, real_label)\n",
    "        \n",
    "        # Combined D loss\n",
    "        loss_d = (loss_d_fake + loss_d_real) * 0.5\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        D.zero_grad()\n",
    "        loss_d.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        #=============== Train the generator ===============#\n",
    "        # First, G(A) should fake the discriminator\n",
    "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "        pred_fake = D.forward(fake_ab)\n",
    "        loss_g_gan = criterionMSE(pred_fake, real_label)\n",
    "\n",
    "        # Second, G(A) = B\n",
    "        loss_g_l1 = criterionL1(fake_b, real_b) * 10\n",
    "        \n",
    "        # loss_g = loss_g_gan + loss_g_l1\n",
    "        loss_g = loss_g_gan \n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "        loss_g.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('======================================================================================================')\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f'\n",
    "                  % (epoch, epoch_range, i, len(train_loader), loss_d.item(), loss_g.item()))\n",
    "            print('======================================================================================================')\n",
    "            show_images(denorm(real_a.squeeze()), denorm(real_b.squeeze()), denorm(fake_b.squeeze()))\n",
    "        \n",
    "        if i > 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(), './min_generator_bw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
